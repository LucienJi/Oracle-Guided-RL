# @package _global_
# Base Adaptive Configuration for DMC Environments
defaults:
  - /oracles/simba_based_oracle_args
  - /oracles/mlp_based_oracle_args

seed: 42
# Default environment configuration (will be overridden by specific environments)
save_video_fps: 30
env:
  render_size: [128, 128]
  n_sub_steps: 1
  seed: ${seed}
  reward_scale: 1.0
  step_penality: 0.01
  sparse_reward: false

# Observation configuration (list shapes will be converted to tuples in code)
obs_config:
  robot_states: [39]     # Robot state observations

training:
  # Basic training settings
  run_name: ${run_name}
  checkpoint_dir: ./checkpoints/${run_name}
  seed: ${seed}
  total_timesteps: 1000000
  replay_dir: ./checkpoints/${run_name}/replay_buffer
  capacity: 1000000
  nstep: 1
  discount: 0.99
  tau: 0.005
  save_episodes_to_disk: true
  
  # Learning starts
  learner_learning_starts: 36000
  per_oracle_explore_steps: 12000
  
  # Action selection and exploration
  action_selection_buffer_size: 10000
  
  
  # Oracle training

  oracle_std_multiplier: 0.25
  
  # Learner training
  policy_delay: 2
  learner_critic_utd_ratio: 2
  learner_critic_batch_size: 1024
  
  # Exploration strategy parameters
  num_proposals: 5
  compile_env_interaction: false
  kappa: 3.0
  # Actor loss weights
  l1_loss_weight: 1.0
  rl_loss_weight: 1.0
  use_adaptive_actor_loss_weight: false
  
  
  # Baseline Parameters
  baseline_greedy_guided: true


  # Soft Max Q - Data Collection Parameters
  sampling_max_temperature: 0.1
  sampling_min_temperature: 0.01
  
  # TD3 temperature 
  min_exploration_noise: 0.01
  max_exploration_noise: 0.1
  target_policy_noise: 0.05
  task_specific_noise: 0.05
  clip_noise: 0.2
  beta: 1.0
  replay_ratio: 0.9

  ## Normalization parameters
  normalize_observations: true
  normalize_rewards: true
  normalized_g_max: 5.0  # Maximum normalized return
  
  ## Categorical critic parameters
  num_bins: 101  # Number of bins for distributional critic
  min_v: -5.0  # Minimum value for categorical distribution
  max_v: 5.0   # Maximum value for categorical distribution
  
  # Optimization parameters
  use_compile: true
  dynamo_suppress_errors: true
  actor_lr: 3e-4
  critic_lr: 3e-4
  value_lr: 3e-4
  actor_weight_decay: 1e-4
  critic_weight_decay: 1e-4
  value_weight_decay: 1e-4
  max_grad_norm: 5.0
  policy_max_grad_norm: 1.0
  warmup_update_times: 2000
  
  # Evaluation and logging
  eval_every: 10000
  eval_episodes: 10
  save_video_every: 10000
  save_freq: 1000
  save_video_fps: ${save_video_fps}
  resume: true
  use_wandb: true
  save_video: true
  wandb_project_name: MW_Oracles
  wandb_entity: jingtianji
  wandb_group: ${method_name}
  log_every: 100
  keep_checkpoint_num: 5

  # For currimaxadv: If we want to halve m times, then we need the max steps should be 2^m 
  # then we can consider "total steps before using the canonical settings", e.g. N
  # the consecutive_steps_halve_every should be N / m 
  max_consecutive_steps: 1
  min_consecutive_steps: 1
  total_consecutive_steps: 200000



rl_agent_args:
  hidden_dim: 256
  num_blocks: 2
  scaler_init: 0.3
  scaler_scale: 1.0
  alpha_init: 0.3
  alpha_scale: 1.0
  c_shift: 4.0
  noise_clip: 0.5  # Clip for truncated normal sampling
  action_dim: 4
  action_high: [1.0, 1.0, 1.0, 1.0]
  action_low: [-1.0, -1.0, -1.0, -1.0]

# SIMBA Critic configuration  
rl_critic_args:
  action_dim: 4
  hidden_dim: 256
  num_blocks: 2
  scaler_init: 0.3
  scaler_scale: 1.0
  alpha_init: 0.3
  alpha_scale: 1.0
  c_shift: 4.0
  num_bins: ${training.num_bins}
  min_v: ${training.min_v}
  max_v: ${training.max_v}
  num_qs: 2  # Number of Q networks (for CDQ)

oracle_critic_args:
  hidden_dim: 256
  num_blocks: 2
  scaler_init: 0.3
  scaler_scale: 1.0
  alpha_init: 0.3
  alpha_scale: 1.0
  c_shift: 4.0
  num_bins: ${training.num_bins}
  min_v: ${training.min_v}
  max_v: ${training.max_v}
  num_qs: 2  # Number of Q networks (for CDQ)
  


# MetaWorld scripted oracle policy defaults (used when oracles_dict entries are "metaworld")
oracle_policy:
  type: "metaworld"
  env_name: ${env.env_name}
  mode: "condition"
  min_noise_scale: 0.03
  max_noise_scales: null
  grid_size: 0.1
  seed: ${seed}
  action_high: 1.0
  action_low: -1.0
  variants: null
  policy_backend: torch
