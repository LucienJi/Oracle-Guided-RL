# @package _global_
# Base MAPS (Max-aggregation Active Policy Selection) configuration for MetaWorld.
defaults:
  - /oracles/simba_based_oracle_args
  - /oracles/mlp_based_oracle_args
seed: 42
save_video_fps: 30

env:
  render_size: [128, 128]
  n_sub_steps: 1
  seed: ${seed}
  reward_scale: 1.0
  step_penality: 0.01
  sparse_reward: false

obs_config:
  robot_states: [39]

training:
  run_name: ${run_name}
  checkpoint_dir: ./checkpoints/${run_name}
  results_root_dir: ./results
  seed: ${seed}
  total_timesteps: 1000000
  replay_dir: ./checkpoints/${run_name}/replay_buffer
  capacity: 1000000
  nstep: 1
  discount: 0.99
  tau: 0.005
  save_episodes_to_disk: true

  # MAPS specific parameters
  maps_beta: 3.0  # UCB exploration coefficient
  roll_in_max_k: 1000  # Maximum roll-in steps (uniformly sampled from [0, k])
  per_oracle_explore_steps: 12000  # Steps per oracle/learner during cycling exploration

  learner_learning_starts: 36000
  learner_critic_utd_ratio: 2
  learner_critic_batch_size: 1024
  policy_delay: 1
  target_policy_noise: 0.1
  task_specific_noise: 0.1
  replay_ratio: 1.0

  # Normalization
  normalize_observations: true
  normalize_rewards: true
  normalized_g_max: 5.0

  # Exploration noise (with decay)
  min_exploration_noise: 0.01
  max_exploration_noise: 0.2
  clip_noise: 0.3
  noise_clip: 0.3

  # Oracle action noise (optional)
  oracle_std_multiplier: [0.2, 0.2]

  # Action selection buffer
  action_selection_buffer_size: 10000

  # Optimizer
  actor_lr: 3e-4
  critic_lr: 3e-4
  value_lr: 3e-4
  actor_weight_decay: 1e-4
  critic_weight_decay: 1e-4
  value_weight_decay: 1e-4
  max_grad_norm: 1.0
  policy_max_grad_norm: 1.0

  # Evaluation / logging
  eval_every: 10000
  eval_episodes: 10
  save_freq: 5000
  save_video: true
  save_video_every: 100000
  save_video_fps: ${save_video_fps}
  resume: true
  use_wandb: true
  wandb_project_name: MW_Oracles
  wandb_entity: jingtianji
  wandb_group: ${method_name}
  log_every: 10
  keep_checkpoint_num: 5

  # Compilation
  use_compile: true
  dynamo_cache_size_limit: 64

  # Distributional critic settings (Simba)
  num_bins: 101
  min_v: -5.0
  max_v: 5.0

# Simba actor (deterministic)
rl_agent_args:
  hidden_dim: 256
  num_blocks: 2
  scaler_init: 0.3
  scaler_scale: 1.0
  alpha_init: 0.3
  alpha_scale: 1.0
  c_shift: 4.0
  noise_clip: ${training.noise_clip}

# Simba critic (distributional)
rl_critic_args:
  hidden_dim: 256
  num_blocks: 2
  scaler_init: 0.3
  scaler_scale: 1.0
  alpha_init: 0.3
  alpha_scale: 1.0
  c_shift: 4.0
  num_bins: ${training.num_bins}
  min_v: ${training.min_v}
  max_v: ${training.max_v}
  num_qs: 2


# MetaWorld scripted oracle policy defaults (used when oracles_dict entries are "metaworld")
oracle_policy:
  type: "metaworld"
  env_name: ${env.env_name}
  mode: "condition"
  min_noise_scale: 0.01
  max_noise_scales: null
  grid_size: 0.1
  seed: ${seed}
  action_high: 1.0
  action_low: -1.0
  variants: null
  policy_backend: torch


